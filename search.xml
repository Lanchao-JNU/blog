<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>odps上自定义UDAF-java开发</title>
    <url>/2021/03/24/odps%E4%B8%8A%E8%87%AA%E5%AE%9A%E4%B9%89UDAF%E5%BC%80%E5%8F%91/</url>
    <content><![CDATA[<p>一:写在前面<br><br>&#160;&#160;&#160;&#160;最近在研究odps的udf开发，不得不说maxcompute对hive的封装真的够狠的，用户需要关心的东西很少。UDF就不多说了，odps和hive没有什么差别，都是继承udf然后重写evaluate()就ok。本文主要介绍一下udaf的开发。<br><br>&#160;&#160;&#160;&#160;首先简单说一下udaf是什么吧。udaf是udf的一个子类。UDF(User Defined Function用户自定义函数)顾名思义就是用户自己用java或者python实现的，封装成一个函数可以到数据库中运行的函数。UDF中还包括UDF，UDAF(User Defined Aggregation),UDTF(User-Defined Table-Generating Functions）。<br><br>&#160;&#160;&#160;&#160;1.UDF指单行输入，单行输出的函数，我们常用的concat(),to_char()都是这样的函数。<br><br>&#160;&#160;&#160;&#160;2.UDAF指多行输入，单行输出的函数，一般和group by连用，常用的有sum(),avg()等等。<br><br>&#160;&#160;&#160;&#160;3.UDAF指单行输入，多行输出的函数，比较少用，用的比较多的有explode()<br><br>&#160;&#160;&#160;&#160;在通常的udaf-java开发流程中，需要继承AbstractGenericUDAFResolver，实现步骤十分复杂，而在odps中，我们只需要继承Aggregator，重点实现 merge(), iterate(), terminate() 这三个方法即可。<br><br>&#160;&#160;&#160;&#160;这里我们用一个Mysql里面十分常见，但是Hive中没有的函数group_concat()函数的UDAF实现来说明开发的流程。<br><br>        （注:本文已经默认你安装了MaxCompute-IDEA所需要的东西并连接了自己的odps,具体过程可见<a href="https://help.aliyun.com/document_detail/50892.html">https://help.aliyun.com/document_detail/50892.html</a>)<br></p>
<p> &#160;&#160;&#160;&#160;二:目的<br><br>&#160;&#160;&#160;&#160;以表A为例子</p>
<table>
<thead>
<tr>
<th>id</th>
<th>device</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>ios</td>
</tr>
<tr>
<td>1</td>
<td>android</td>
</tr>
<tr>
<td>2</td>
<td>ios</td>
</tr>
<tr>
<td>1</td>
<td>ios</td>
</tr>
</tbody></table>
<p>&#160;&#160;&#160;&#160;实现如下聚合:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> id,group_concat(device) <span class="keyword">from</span> tableA <span class="keyword">group</span> <span class="keyword">by</span> id</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr>
<th>id</th>
<th>device</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>ios,android,ios</td>
</tr>
<tr>
<td>2</td>
<td>ios</td>
</tr>
</tbody></table>
<p>&#160;&#160;&#160;&#160;三.具体实现:<br><br>&#160;&#160;&#160;&#160;根据mapreduce的工作原理，我们可以画一张流程图。把iterate想象成map()函数，merge想象成combine(),terminate想象成reduce()函数。</p>
<p><img src="/images/pasted-1.png" alt="upload successful"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">代码如下</span><br><span class="line">package com.yupaopao.udaf;</span><br><span class="line"></span><br><span class="line">import com.aliyun.odps.io.Text;</span><br><span class="line">import com.aliyun.odps.io.Writable;</span><br><span class="line">import com.aliyun.odps.udf.UDFException;</span><br><span class="line">import com.aliyun.odps.udf.Aggregator;</span><br><span class="line">import com.aliyun.odps.udf.annotation.Resolve;</span><br><span class="line">import java.io.DataInput;</span><br><span class="line">import java.io.DataOutput;</span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * 定义输入输出类型</span><br><span class="line"> *&#x2F;</span><br><span class="line">@Resolve(&#123;&quot;String-&gt;String&quot;&#125;)</span><br><span class="line">public class GroupConcat extends Aggregator &#123;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 继承Writable接口 自定义buffer进行序列化</span><br><span class="line">     *&#x2F;</span><br><span class="line">    private static class GroupConcatBuffer implements Writable &#123;</span><br><span class="line">        private String str &#x3D; &quot;&quot;;</span><br><span class="line">        @Override</span><br><span class="line">        public void write(DataOutput out) throws IOException &#123;</span><br><span class="line">            out.writeChars(str);</span><br><span class="line">        &#125;</span><br><span class="line">        &#x2F;**</span><br><span class="line">         * Writable的readFields方法， 由于partial的writable对象是重用的，</span><br><span class="line">         * 同一个对象的readFields方法会被调用多次。该方法每次调用的时候重置整个对象，如果对象中包含Collection，则需要清空。</span><br><span class="line">         *&#x2F;</span><br><span class="line">        @Override</span><br><span class="line">        public void readFields(DataInput in) throws IOException &#123;</span><br><span class="line">            str &#x3D; in.readLine();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 声明最终结果变量</span><br><span class="line">     *&#x2F;</span><br><span class="line">    private Text ret &#x3D; new Text();</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 创建初始返回结果的值</span><br><span class="line">     *&#x2F;</span><br><span class="line">    @Override</span><br><span class="line">    public Writable newBuffer() &#123;</span><br><span class="line">        return new GroupConcat.GroupConcatBuffer();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * @param buffer 为一个阶段的汇总数据(在不同map()中汇总出来的group by数据)</span><br><span class="line">     * @param args 表示一行数据 其中args[0]表示传入的第一列(一般仅传入一列)</span><br><span class="line">     * 每次新增一条数据 就把传入的列放入目前已有的累积字符串中。</span><br><span class="line">     *&#x2F;</span><br><span class="line">    @Override</span><br><span class="line">    public void iterate(Writable buffer, Writable[] args) throws UDFException &#123;</span><br><span class="line">        Text arg &#x3D; (Text) args[0];</span><br><span class="line">        GroupConcat.GroupConcatBuffer buf &#x3D; (GroupConcat.GroupConcatBuffer) buffer;</span><br><span class="line">        if (arg !&#x3D; null) &#123;</span><br><span class="line">            buf.str&#x3D;buf.str+arg+&quot;,&quot;;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;**</span><br><span class="line">     * @param buffer  聚合buffer</span><br><span class="line">     * @param partial 其他分片聚合结果</span><br><span class="line">     * 在这个方法中，对不同map()的结果进行汇总</span><br><span class="line">     *&#x2F;</span><br><span class="line">    @Override</span><br><span class="line">    public void merge(Writable buffer, Writable partial) throws UDFException &#123;</span><br><span class="line">        GroupConcat.GroupConcatBuffer buf &#x3D; (GroupConcat.GroupConcatBuffer) buffer;</span><br><span class="line">        GroupConcat.GroupConcatBuffer p &#x3D; (GroupConcat.GroupConcatBuffer) partial;</span><br><span class="line">        buf.str &#x3D; buf.str+ p.str;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 设置最终结果，同时把留在最后面的逗号去掉。</span><br><span class="line">     * @param buffer</span><br><span class="line">     * @return Object UDAF的最终结果</span><br><span class="line">     *&#x2F;</span><br><span class="line">    @Override</span><br><span class="line">    public Writable terminate(Writable buffer) throws UDFException &#123;</span><br><span class="line">        GroupConcatBuffer buf &#x3D; (GroupConcatBuffer) buffer;</span><br><span class="line">        if (buf.str&#x3D;&#x3D;&quot;&quot;)&#123;</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            String result &#x3D; buf.str.substring(0, buf.str.length()-1);</span><br><span class="line">            ret.set(result);</span><br><span class="line">        &#125;</span><br><span class="line">        return ret;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>&#160;&#160;&#160;&#160;四:<br><br>&#160;&#160;&#160;&#160;打包右键点击写好的java文件，选择deploy to server打包<br></p>
<p><img src="/images/pasted-3.png" alt="upload successful"></p>
<p>&#160;&#160;&#160;&#160;点击右上方的MaxCompute，选择添加资源，把资源上传到odps<br></p>
<p><img src="/images/pasted-4.png" alt="upload successful"></p>
<p>&#160;&#160;&#160;&#160;点击右上方的MaxCompute，选择创建UDF，起个名字，把udf注册进去(注意:这里使用的名字就是最后在odps里使用的名)<br><br><img src="/images/pasted-5.png" alt="upload successful"></p>
<p>&#160;&#160;&#160;&#160;五:<br><br>&#160;&#160;&#160;&#160;函数使用。成功！<br><br> <img src="/images/pasted-2.png" alt="upload successful"></p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>最终幻想7重置版--写在通关后的第329天(涉及剧透)</title>
    <url>/2021/03/26/%E6%9C%80%E7%BB%88%E5%B9%BB%E6%83%B37%E9%87%8D%E7%BD%AE%E7%89%88-%E5%86%99%E5%9C%A8%E9%80%9A%E5%85%B3%E5%90%8E%E7%9A%84%E7%AC%AC329%E5%A4%A9/</url>
    <content><![CDATA[<p>&#160;&#160;&#160;&#160;2020年在我的游戏史中注定是浓墨重彩的一年。初期疫情影响，后期进入职场，让我有大把时间体验电子游戏，这一年玩到的佳作也特别多:女神异闻录5，异度神剑2，如龙7，勇者斗恶龙11S等等等等，但是丝毫不影响最终幻想7重置版的出彩。</p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-12.png width = "75%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">诞生之前就已是传说</font> 
</center>
&#160;&#160;&#160;&#160;老实说，我完全跟最终幻想7老玩家这个群体沾不上边，没有办法想象我认识的人中会有系列玩家。和最终幻想7最早的交集是2016年上大学买了游戏本在游民星空的游戏库里搜寻盗版单机游戏的时候，发现明明最终幻想已经出到十几了，这个七代还是在好评榜前列，但是当时搜了一下发现这游戏甚至是上个世纪开发的东西，就完全提不起兴趣。<br>
&#160;&#160;&#160;&#160;接下来，已经到了2020年，我稍微玩了点单机大作，也稍微会关注游戏资讯的时候。这两年间，重制版大作层出不穷，我记得很清楚，当时在涩谷过马路的时候，一抬头就可以看见生化危机2重制版的巨幅海报。同年还有魔兽争霸3重制版，生化危机3重制版等等重制版大作。所以对我来说「最终幻想7重制版-第一章」只不过是另一款跟其他情怀大作没有什么区别的重制游戏罢了，再加上这个「第一章」的副标题，在我这里刻上了像生化危机3般又短情怀又不够的标签。
<br>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-8.png width = "75%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">生化危机2重制版海报</font> 
</center>
&#160;&#160;&#160;&#160;然后是游戏发售当天，以a9vg为首的游戏媒体转载了游戏的宣传片，宣传片中蒂法，爱丽丝的建模确实非常惊艳，战斗系统的演示也不差，才让我有一点点对这个游戏改观。最后击中我的是一张情怀浓浓的图片，图片里九头身高清克劳德牵着像素小人克劳德，旁白"Let's save the world again"。那晚上我转发了这条微博后立马打开psn购入了这款游戏，然后就是50+个小时，20天的unknown journey。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-13.png width = "45%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">Let's save the world again</font> 
</center>
&#160;&#160;&#160;&#160;跟大行其道的主流3A比，「最终幻想7重制版-第一章」并不算一款快节奏的游戏，即便有接近欧美厂商的画面技术和全球宣发造势，它在内核上还是有相当深刻JRPG的影子：节奏缓慢，人物美型，追求数值而策略而不是操作，甚至很多支线小任务的设计也存在着和JRPG一致的弊端。<br>
&#160;&#160;&#160;&#160;在剧情上，我虽然是新玩家，但是购入游戏等待下载的时候，我已经在知乎/贴吧/游戏媒体等地大致了解了原作剧情:包括神罗公司的背景，爱丽丝之死，克劳德的情况，甚至连CC里面扎克斯的故事和后传圣子降临有了解。坦白说最终幻想的主线剧情，讨论星球和生命的议题，牺牲和平等的议题，在1997年可能非常新颖，但是经过20多年影视作品也好，其他游戏也好，已经把同样的议题翻来覆去讨论了太多遍。如果只说第一章的话，其实就是一个简单的环保组织反抗过度开采星球能源神罗公司的故事，其中米德加的游戏背景，上层和下层的生活情况和贫富差别很像「铳梦」，最近也在电影「阿丽塔：战斗天使」里呈现了。因此，本身「最终幻想7重制版-第一章」的剧情，应该是不太值得期待的。但是野村哲也这个鬼才，可以说是重新定义了重制。既然老玩家都知道剧情，原来的剧情可能由于时代原因难以吸引新玩家，那就把重制版做成戏说最终幻想7。游戏主线剧情不变，加入了元素「菲拉」暗示角色命运的改变，每次角色即将做出和原作剧情不符合的行为的时候，「菲拉」就会跳出来阻止并触发战斗，把角色的行为纠正回原作。其实在游戏中，从结果看「菲拉」的加入对原作的剧情没有改动，但是当菲拉被打败后，却引入了新的可能性：角色的命运即将改写，1997年的最终幻想7已经无法阻止他们了。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-14.png width = "45%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">命运修正者:菲拉</font> 
</center>
&#160;&#160;&#160;&#160;为什么野村哲也会想到用命运改变的形式来重制剧情？我认为最大的原因是，最终幻想7中爱丽丝的死亡和最终幻想7核心危机中扎克斯的死亡，可能是系列玩家心中最大的遗憾，在这上面做文章，最容易俘获老玩家的心。<br>
&#160;&#160;&#160;&#160;先讲讲爱丽丝之死，在1997年，3D游戏的先驱Square Enix第一次将大型3D游戏投入市场的时候，游戏大体还是青少年的消费品，剧情停留在魔法与剑，骑士与公主的故事，主要用文字刻画。在这些少年少女眼里，爱丽丝在游戏前半段是伙伴，是团队里唯一的奶妈，是注定和主角团一起迎接胜利的角色。同时，游戏前半段也花费了大幅的篇幅去刻画爱丽丝的性格，从教堂相遇到Forgotten City，一步步让玩家喜欢上这个像素3D小人。在游戏中间，用萨菲罗斯一把从天而降的大剑在玩家面前刺死爱丽丝，同时Aerith's Theme响起，玩家与萨菲罗斯战斗后播放克劳德水葬爱丽丝cg，而后一张碟结束。玩家起身到电视机前去换碟，给了悲伤驻足的时间。有趣的是，爱丽丝之死是真正的死亡，没有复活，JRPG的玩家应该都知道奶妈的重要性，爱丽丝一离队对游戏后半程的体验肯定是有影响的，制作者选择让爱丽丝担任这个角色，也是在游戏后半段一遇到血量困难的情况，会想起来“如果爱丽丝在就好了”。从结果上看，爱丽丝之死的设计无疑是成功的，全球无数玩家都在谣传复活爱丽丝的方法，我本人也在网路上看到诸如把碟换回来换回去就能复活爱丽丝的谣言。那么，当角色的命运改变，是不是就给了爱丽丝之死新的可能性？当年在全球玩家口中的谣言，会不会变成真的？我想只有野村哲也和北范佳濑知道了。
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-15.png width = "45%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">爱丽丝之死</font> 
</center>]]></content>
      <categories>
        <category>Game</category>
      </categories>
      <tags>
        <tag>final fantasy</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop-MapReduce流程</title>
    <url>/2021/03/26/Hadoop-MapReduce%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<p>&#160;&#160;&#160;&#160;本文主要放一下之前学习mapreduce的笔记。也给自己创造一个再次阅读，巩固的机会。首先声明：本人看的Hadoop源码不多，内容主要摘自各博客，然后自己总结精简得出<br><br>￼<br><img src="/images/pasted-6.png" alt="upload successful"><br>&#160;&#160;&#160;&#160;MapReduce包括Mapper（Mapper类)阶段和Reducer(Reducer类)阶段，其实说白了就是两个实现类，其中Map阶段和Reduce阶段都包含部分Shuffle阶段工作，因此也有人把mapreduce流程说成map-&gt;shuffle-&gt;reduce。</p>
<p><font size=5><strong>Map阶段</strong></font><br>&#160;&#160;&#160;&#160;1.输入分片 input split: 一个大的文件会根据block块切分成多个分片(这里生成多少个分片可以是用户自定义的，只需要通过hive参数设定每个block的大小即可)。每个输入分片会让一个map进程来处理。<br>&#160;&#160;&#160;&#160;2.Map任务:<br>&#160;&#160;&#160;&#160; i. 初始化: 创建context，map.class实例，设置输入输出，创建mapper的上下文Map 任务把分片传递给 TaskTracker 里面的 InputFormat类的 createRecordReader() 方法以便获取分片的 RecordReader 对象,由RecordReader读取分片数据并转成键值对形式。<br>&#160;&#160;&#160;&#160;ii. 执行:  执行 Mapper class.的run()方法，这个方法可以被重写，对每个分片里的数据执行一样的方法。(比如说 给每个key生成一个value=1)<br>3.shuffle(Map端):<br>&#160;&#160;&#160;&#160;i.溢写:map会使用Mapper.Context.write()将map函数的输出溢写到内存中的环形缓冲区，同时调用Partitioner类对数据分区，每一个分区对应一个reducer。当缓冲区即将充满(80%)，溢写会被执行(并行，缓冲区可以继续写入，当溢写未完成而缓冲区已满，map会停止写入内存)具体过程<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;a.创建一个溢写记录SpillRecord 和一个FSOutputStream 文件输出流（本地文件系统）<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;b.内存内排序缓冲中的块：输出的数据会使用快排算法按照partitionIdx, key排序<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;c.分区序列化写到磁盘,序列化(快排)之前可进行combiner(可选)，方便排序<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;d.merge合并溢写文件,如果合并后文件较大，可以再进行一次combiner<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;e.再进行一次Partitioner对数据分区 (为什么要两次呢,因为这个时候可能进行了两次combiner,应该分配到每个reducer上的数据出现变化)<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;ii. combiner合并阶段(可选,有两次触发时机):简单地合并key值<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;a.减少传到reducer的数据量（即通过减少key-value对减少网络传输)<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;b.减轻reducer的负担<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;c.缺点 job没有依赖，有磁盘io开销<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;iii.Partitioner(补充): <br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;a.Hadoop Mapreduce默认的Partitioner是Hash Partitioner,对key计算hash值并分区<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;b.TotalOrderPartitioner 按照数据大小分区 常用于全局排序 按照大小将数据分成多个区间，后一个区间的数据一定大于前一个区间<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;c. Partitioner可以被重写，克服低效分区，解决数据倾斜和全局排序问题<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;iv. 排序原因:<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;a.方便reducer找到自己分区的数据<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; b.减轻reducer中排序负担<br><br><font size=5><strong>Reduce阶段</strong></font><br>&#160;&#160;&#160;&#160;1.shuffle(Reduce端):<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;i.copy: Reduce端并行的从多个map下载该reduce对应的partition部分数据（通过HTTP)<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;ii.merge(内存到磁盘): Reduce将map结果下载到本地时，需要把多个mapper输入流合并成一个,一直在进行，直到从map下载数据完毕。<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;iii.split: Reduce对下载下来的map数据，会先缓存在内存中,当内存达到一定用量时写入硬盘(这里也可以进行combiner)<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;iv.merge(磁盘到磁盘)：map数据下载过程中，一个后台线程把这些文件合并为更大的，有序的文件，也就是进行全局排序，为最后的最终合并减少工作量(多轮)<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;v. 最终merge(方式不确定): map数据下载完毕后，所有数据被合并成一个整体有序的文件作为Reduce的输入。有可能是都来自于硬盘，有可能都来自于内存，也有可能两边都有<br>&#160;&#160;&#160;&#160;2.Reduce任务:<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;i.自己实现逻辑 Map阶段的map方法类似 通常做聚合操作<br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;ii.OutputCollector.collect() 方法把 reduce 任务的输出结果写到 HDFS</p>
]]></content>
      <categories>
        <category>BigData</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>2021-03-29 周报</title>
    <url>/2021/03/29/2021-03-29-%E5%91%A8%E6%8A%A5/</url>
    <content><![CDATA[<p>&#160;&#160;&#160;&#160;这周租的房间城管拆掉，无奈回家了一趟，目前居住酒店，所以可以写的内容也会比较少。<br><br><font size=5><strong>Big Data</strong></font><br><br>&#160;&#160;&#160;&#160;1.阅读「数据仓库工具箱」「仓库」章节，深入了解缓慢变化维的问题背景和解决方式，企业数据仓库总线架构。<br><br>&#160;&#160;&#160;&#160;2.了解MPP的OLAP系统Doris，脱胎于百度的Palo,在2018年捐献给Apache基金会。doris使用场景与Clickhouse相似(处理高质量的结构化数据)，但是特性和原理都有所不同。<br></p>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-16.png width = "75%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">主流OLAP引擎对比1</font> 
</center>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    src=/images/pasted-17.png width = "75%" alt=""/>
    <font style="font-size:14px;color:#C0C0C0;text-decoration:underline">主流OLAP引擎对比2</font> 
</center>

<p>&#160;&#160;&#160;&#160;可以看到Clickhouse和Doris都能支撑OLAP需要的大部分场景，但是Clickhouse的并发量一直是个问题。同时Doris支持Mysql访问协议，跟clickhouse的非标协议接口相比，上手起来会更加舒服。但是也需要注意，Doris在2018年开始开源，目前社区非常不活跃，遇到问题可能难以寻求帮助。搭建上我感觉差不多麻烦吧，卧龙凤雏。<br><br>&#160;&#160;&#160;&#160;顺便也讲讲原理差别，除了OLAP常见的列存储，向量化执行外，Doris还有多数据模型(预聚合，K-V)的特点，为不同查询场景定制。而Clickhouse的编码压缩，多索引比较牛逼。<br><br><font size=5><strong>Game</strong></font><br><br>&#160;&#160;&#160;&#160;1.本周购买了「MONSTER HUNTER RISE」，之前其实玩过「MONSTER HUNTER WORLD」比较失望，只玩了差不多一小时就没动过了，所以这次买的ns卡带，还没有发货。<br><br>&#160;&#160;&#160;&#160;2.非常想玩双人成行，Josef Fares的游戏「双子传说」「逃出生天」都非常有意思，目前也算是我觉得个人风格最吸引人的游戏制作人，嘴挺臭但是确实有两把手的。可是这游戏需要两人玩，然后拥有PS4/PS5的人还比较少，慢慢等吧。<br></p>
<p><font size=5><strong>Others</strong></font><br><br>&#160;&#160;&#160;&#160;1.这周终于把博客建起来了，用的是Hexo+Next，使用体验还行,准备把以前沉淀的一些文章也搬过来，后续要坚持一下。<br><br>&#160;&#160;&#160;&#160;2.做题进度:<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;#剑指 Offer 31 栈的压入、弹出序列<br><br>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;#剑指 Offer 32 - I 从上到下打印二叉树</p>
]]></content>
  </entry>
</search>
